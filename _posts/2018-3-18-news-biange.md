---
layout:     post
title:     变革尚未成功：深度强化学习研究的短期悲观与长期乐观
subtitle:   ""
date:       2018-03-18
author:     " "
header-img: "img/home-bg-o.jpg"
tags:
    - 新闻
---

本文主要内容：深度强化学习是最接近于通用人工智能（AGI）的范式之一。不幸的是，迄今为止这种方法还不能真正地奏效。在本文中，作者将为我们解释深度强化学习没有成功的原因，介绍成功的典型案例，并指出让深度强化学习奏效的方法和研究方向。





<!-- more -->

## 主要内容

本文所引文献大多数来自于 Berkeley、Google Brain、DeepMind 以及 OpenAI 过去几年的工作，因为它们更容易获得。我难免遗漏了一些比较古老的文献和其他研究机构的工作，我表示很抱歉——毕竟一个人的时间精力有限。

我曾经在 Facebook 说过：

> 当别人问我强化学习能否解决他们的问题时，至少有 70% 的时候我的回答是：不能。

![images](/images\news\2018-3-18-biange.jpg)

深度强化学习被成堆的炒作包围着，并且都有足够好的理由！强化学习是一种难以置信的通用范式，原则上，一个鲁棒而高性能的强化学习系统可以处理任何任务，而且将这种范式和深度学习的经验学习能力相结合是很自然的。深度强化学习是最接近于通用人工智能（AGI）的范式之一。

不幸的是，它目前还不能真正地奏效。

但现在，我相信它会取得成功的。如果我不相信强化学习，我是不会从事相关工作的。但是在通往成功的路上存在很多问题，而且很多问题从根本上来说都是很困难的。智能体的漂亮 demo 背后隐藏着创造它们的过程中所付出的所有心血、汗水和泪水。

我多次看到人们被最新的研究所吸引，他们初次尝试使用深度强化学习，而且没有失败，于是低估了深度强化学习所面临的困难。毫无疑问，「玩具问题」并不像看起来那么简单。无一例外，这个领域会数次「摧残」他们，直至他们学会设定更现实的研究期望。

实际上这并不是任何人的错，它更像是一个系统问题。讲述积极结果的故事是很容易的，但承认消极的结果是很困难的。问题在于消极的结果是研究者最常遇到的。某种程度上，消极的结果实际上比积极的结果更加重要。

在这篇文章的其余部分，我会解释一下深度强化学习没有成功的原因，它成功的典型案例，以及将来让深度强化学习更加可靠地工作的方式。我相信如果在这些问题上可以达成一致，并实实在在地讨论相关的问题，而不是独立地重复地去一次又一次地重新发现相同的问题。

我希望看到更多的关于深度强化学习的研究。我希望有新人加入这个研究领域，我也希望知道新人们能够了解他们正在做什么。

在开始文章的剩余部分之前，有几点提示：

- 我在这篇文章中引用了一些论文。通常，我会因其令人信服的负面例子而引用一篇论文，而不引用正面例子。这并不意味着我不喜欢那些论文。我喜欢这些论文，如果有时间的话，它们是值得一读的。

- 我在这篇文章中可互换地使用「reinforcement learning，强化学习」和「deep reinforcement learning，深度强化学习」，因为在我的日常工作中，强化学习一直蕴含着深度强化学习的意思。我所批判的是深度强化学习的经验行为，而不是一般的强化学习范式。我所引用的论文中通常使用了深度神经网络的智能体。尽管这种经验批判可能同样适用于线性强化学习或者列表格式强化学习，但是我并不认为这也适用于到更小的问题。强化学习有望被应用于大型、复杂、高维的环境中，在这些环境中良好的函数逼近是必要的。受此驱动，人们才炒作强化学习，这些炒作正是需要重点解决的问题。

- 这篇文章的基调是由悲观向乐观转换的。我知道文章有些长，但是我更希望你花点时间读完全文再做回复。

[文章未完，点击这里查看全文（来自机器之心）](http://mp.weixin.qq.com/s/5fuiigX9OsEhrTMviJbQDQ)