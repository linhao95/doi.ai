---
layout:     post
title:      清华&斯坦福提出深度梯度压缩DGC，大幅降低分布式训练网络带宽需求
subtitle:   ""
date:       2018-03-16
author:     " "
header-img: "img/home-bg-o.jpg"
tags:
    - 新闻
---

本文主要内容：来自清华大学和斯坦福大学的研究者们发现，分布式随机梯度下降训练中 99.9% 的梯度交换都是冗余的——通过他们提出的深度梯度压缩（DGC）方法，神经网络训练可以大幅降低通信带宽需求。在多个基准模型上的对比实验表明，该方法可以在不降低准确率的情况下达到 270 倍到 600 倍的梯度压缩率，使得小带宽甚至移动设备上的大规模分布式训练变为可能。





<!-- more -->

## 主要内容

**作者简介**

>林宇鋆是清华大学电子工程系 NICS 实验室 2014 级本科生，于 2017 年暑假在斯坦福参加暑研期间同韩松博士一起出色完成了 DGC 的工作，收到 MIT, Stanford, CMU, UMich 等美国名校的博士项目录取，并将于 2018 年秋加入 MIT HAN Lab 攻读博士学位。韩松博士于 2017 年毕业于斯坦福大学，师从 GPU 之父 Bill Dally 教授。他的研究涉足深度学习和计算机体系结构，他提出的 Deep Compression 模型压缩技术曾获得 ICLR 2016 最佳论文，ESE 稀疏神经网络推理引擎获得 FPGA 2017 最佳论文，引领了世界深度学习加速研究，对业界影响深远，于博士期间联合创立了深鉴科技。基于对一系列重要科研成果的继续深入探索，韩松博士将于 2018 年任职 MIT 助理教授，创立 HAN Lab。


大规模分布式训练可提升训练大型深度模型的训练速度。同步随机梯度下降（SGD）已被普遍用于分布式训练。通过增加训练节点数量，利用数据并行化的优势，我们能够极大地减少在相同规模的训练数据上做前向-后向传播的计算时间。然而，分布式训练中梯度交换的成本过高，尤其是在计算-通信比率较低的循环神经网络（RNN）等情况下；由并行训练带来的计算时间上的节省，可能将不足以补偿通信时间上代价的增长。因此，网络带宽成为了分布式训练规模化的最大瓶颈。特别是在移动设备上做分布式训练时，带宽问题变得更加显著，例如联合学习（federated learning）。因为具备更好的隐私性、个性化等特点，在移动设备上训练神经网络模型变得更加诱人，但其面临的重大挑战包括移动设备网络中的更低的带宽、不连贯的网络连接、价格昂贵移动数据流量等问题。

![images](/images\news\2018-3-16-DGC-1.jpg)

深度梯度压缩（Deep Gradient Compression，DGC）通过压缩梯度解决了通信带宽问题，如图 1 所示。为了确保没有损失准确率，DGC 在梯度稀疏化之上应用了动量修正（momentum correction）和局域梯度修剪（local gradient clipping）方法。DGC 还应用了动量因子掩蔽（momentum factor masking）和预热训练（warm-up training）以克服通信量减少带来的陈化问题。

研究者通过实验在多种任务、模型和数据集上验证了 DGC 的有效性：

- CNN 网络：图像分类任务，CIFAR-10 和 ImageNet 数据集；

- RNN 网络：语言建模任务，Penn Treebank 数据集；

- 语音识别任务，Librispeech Corpus。

这些实验证明了我们可以对梯度进行 600 倍的压缩而不损失准确率，相比于之前的研究（Aji & Heafield, 2017），DGC 的性能提升了一个量级。

[文章未完，点击这里查看全文（来自机器之心）](http://mp.weixin.qq.com/s/fx0Pfu0MOPjSkzi5mL6U_A)